{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7fb61d0",
   "metadata": {},
   "source": [
    "Word Embeddings is the process to represent the words for text analysis as a real valued vector so that similar words or vectors will be closer to each other\n",
    "\n",
    "They are of 2 types , \n",
    "    (1)count or frequency -> One hot encoding, bag of words, TF-IDF\n",
    "    (2) Deep learning models -> Word2Vec\n",
    "        (A)CBow (continious bag of words ANN ) \n",
    "        (B)Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f197fcd",
   "metadata": {},
   "source": [
    "Word2Vec:\n",
    "this uses neural netwrok to learn the word associations from a large corpus of text, once trained the model detects synonymus words for the partial sentence.\n",
    "As the name suggest word2vec represents each word with a list of number called a vector.\n",
    "\n",
    "a) CBOW(Continous Bag of Words):\n",
    "    -> We take a window size containg the number of words, and then a center word of the window\n",
    "    -> Each time we take the window and keep it moving one word at a time and repeat the process and then train the model\n",
    "    -> CBOW is a fully connected Neural network , from there we train the weights and then see how they move and work on the loss function and backward propogation  \n",
    "    -> We use this for a small set of corpus\n",
    "\n",
    "b) Skipgram:\n",
    "    -> The input output has been changed, this is done to reduce the size of the sparsity of the matrix\n",
    "    -> We use this for a large corpus of words \n",
    "\n",
    "Advantages of Word2Vec:\n",
    "    -> This makes a dense Matrix \n",
    "    -> Semantic Meaining of the words are captured and the similarity is also captured\n",
    "    -> Vocabulary size is fixes [dimentions are around 300]\n",
    "    -> Out of vocabulary is also solved as we have a huge corpus and almost every word is captured\n",
    "\n",
    "\n",
    "Average Word2Vec:\n",
    "    For the entire sentence each individual word will have a representation and we will take the corresponding avg of each row to create the avg of all the words to represent the sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using google pre-trained models:\n",
    "#https://huggingface.co/fse/word2vec-google-news-300\n",
    "\n",
    "# !pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "vec_king=wv['king'] #this will take the word and convert ttto a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f422394",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('cricket') #This is used to find the words most similar to the word cricket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c61d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.similarity(\"hockey\", \"sports\") #this is to tell how similar the two words are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd814bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=wv['king']-wv['man']+wv['woman'] #here this prooves that we are able to use the model to convert as numerical values\n",
    "wv.most_similar([vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f8c68",
   "metadata": {},
   "source": [
    "Spam And Ham => using Bag Of Words and tfidf to convert the text to numerical values and then use machine learning to perform the classication of Spam or not (Ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a44c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "messages=pd.read_csv('SMSSpamCollection.csv',sep='\\t',names=[\"label\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703330aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Clearning And Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f98740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "\n",
    "corpus=[]\n",
    "for i in range(0,len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d059f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The output feature is the labels\n",
    "\n",
    "y=pd.get_dummies(messages['label']).astype(int)\n",
    "y #now instead of having two various columns we can use only one of them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19632ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.iloc[:,1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ccfbd",
   "metadata": {},
   "source": [
    "Note as a best practice we need to follow the below steps:\n",
    "1) Preprocessing and Cleaning \n",
    "2) Train and Test\n",
    "3) BOW and TFIDF -> This is done to prevent any data leakage\n",
    "4) Trained the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(corpus,y,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa821f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=100,binary=True, ngram_range=(1,2)) # place binary=True , if we should use Binary BOW\n",
    "X_train=cv.fit_transform(X_train).toarray() #X is the independent features\n",
    "X_test=cv.transform(X_test).toarray() #X is the independent features\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f40c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #this performs well on sparse matrices\n",
    "spam_detect_model = MultinomialNB().fit(X_train,y_train)\n",
    "y_pred=spam_detect_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c12c4f",
   "metadata": {},
   "source": [
    "#Spam and Ham Project using TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d78560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(corpus,y,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda8c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(max_features=2055,ngram_range=(1,2))\n",
    "X_train=tfidf.fit_transform(X_train).toarray()\n",
    "X_test=tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #this performs well on sparse matrices\n",
    "spam_tfidf_detect_model = MultinomialNB().fit(X_train,y_train)\n",
    "y_tfidf_pred=spam_tfidf_detect_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "accuracy_score(y_test,y_tfidf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cbe3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_tfidf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232480e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
